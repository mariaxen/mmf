diff --git a/mmf/configs/defaults.yaml b/mmf/configs/defaults.yaml
index 14ebaf3..256bd16 100644
--- a/mmf/configs/defaults.yaml
+++ b/mmf/configs/defaults.yaml
@@ -14,18 +14,18 @@ training:
     seed: -1
     # Name of the experiment, will be used while saving checkpoints
     # and generating reports
-    experiment_name: run
+    experiment_name: mmf_video_audio
     # Maximum number of iterations the training will run
     max_updates: 22000
     # Maximum epochs in case you don't want to use max_updates
     # Can be mixed with max iterations, so it will stop whichever is
     # completed first. Default: null means epochs won't be used
-    max_epochs: null
+    max_epochs: 25
 
     # After `log_interval` updates, current update's training loss will be
     # reported. This will also report validation on a single batch from validation set
     # to provide an estimate on validation side
-    log_interval: 100
+    log_interval: 1
     # Level of logging, only logs which are >= to current level will be logged
     logger_level: info
     # Log format: json, simple
@@ -47,7 +47,7 @@ training:
     # Weights and Biases control, by default Weights and Biases (wandb) is disabled
     wandb:
         # Whether to use Weights and Biases Logger, (Default: false)
-        enabled: false
+        enabled: true
         # An entity is a username or team name where you're sending runs.
         # This is necessary if you want to log your metrics to a team account. By default
         # it will log the run to your user account.
@@ -79,7 +79,7 @@ training:
     # KxN (without incurring the memory overhead of setting batch_size to KxN).
     update_frequency: 1
     # Number of workers to be used in dataloaders
-    num_workers: 4
+    num_workers: 2 #4
     # Some datasets allow fast reading by loading everything in the memory
     # Use this to enable it
     fast_read: false
@@ -93,10 +93,10 @@ training:
 
     # After `checkpoint_interval` number of updates, MMF will make a snapshot
     # which will involve creating a checkpoint for current training scenarios
-    checkpoint_interval: 1000
+    checkpoint_interval: 285 #1000
     # This will evaluate evaluation metrics on whole validation set after
     # evaluation interval number of updates
-    evaluation_interval: 1000
+    evaluation_interval: 1 #1000
     # Whether gradients should be clipped
     clip_gradients: false
     # Mode for clip norm
@@ -137,7 +137,7 @@ training:
     # Warmup factor learning rate warmup
     warmup_factor: 0.2
     # Iteration until which warnup should be done
-    warmup_iterations: 1000
+    warmup_iterations: 340
 
     # Device on which the model will be trained. Set 'cpu' to train/infer on CPU
     device: cuda
@@ -153,7 +153,7 @@ training:
 
     # By default metrics evaluation is turned off during training. Set this to true
     # to enable evaluation every log_interval number of updates
-    evaluate_metrics: false
+    evaluate_metrics: true
 
     # This will enable anomaly detection mode in PyTorch. Use this for debugging
     # purposes if you see NaN issues in your experiments.
@@ -164,7 +164,7 @@ training:
     # FP16 support through torch.cuda.amp autocast and grad scaler.
     # Set to true to activate fp16 for faster performance with negligible
     # drop in results.
-    fp16: false
+    fp16: true
 
     # Users can define their own callback functions in the trainer, e.g. adjust
     # learning rate, plot data in tensorboard, etc.
@@ -189,19 +189,19 @@ trainer:
     # https://pytorch-lightning.readthedocs.io/en/latest/trainer.html#trainer-class-api
     type: lightning
     params:
-        gpus: 1
+        gpus: 8
         num_nodes: 1
         precision: 32
         deterministic: false
         benchmark: false
         max_steps: 22000
-        max_epochs: null
+        max_epochs: 10
         gradient_clip_val: 0.0
         num_sanity_val_steps: 0
         enable_checkpointing: true
         accumulate_grad_batches: 1
-        val_check_interval: 1000
-        log_every_n_steps: 100
+        val_check_interval: 5
+        log_every_n_steps: 10
         logger: false
         limit_val_batches: 1.0
         # progress bar is turned off if want same logging format as `mmf_trainer`
@@ -222,14 +222,14 @@ trainer:
 # Configuration for evaluation
 evaluation:
     # Metrics for evaluation
-    metrics: []
+    metrics: ["accuracy", "specificity", "sensitivity", "npv", "ppv", "confusion_matrix"]
     # Use CPU for metrics and other calculations, you can use this option if
     # you see OOM in validation as in metrics are calculated globally
-    use_cpu: false
+    use_cpu: true
     # Generate predictions in a file
-    predict: false
+    predict: true
     # Prediction file format (csv|json), default is json
-    predict_file_format: json
+    predict_file_format: csv
     # Test reporter params. Defaults to type: file
     reporter:
         type: file
@@ -257,7 +257,7 @@ config: null
 # Type of run, train+inference by default means both training and inference
 # (test) stage will be run, if run_type contains 'val',
 # inference will be run on val set also.
-run_type: train_inference
+run_type: train_val
 
 # Configuration for optimizer, examples can be found in models' configs in
 # configs folder
@@ -299,27 +299,27 @@ env:
     # Directory for saving logs, default is "logs" inside the save folder
     # If log_dir is specifically passed, logs will be written inside that folder
     # Use MMF_LOG_DIR or env.log_dir to override
-    log_dir: ${oc.env:MMF_LOG_DIR,}
+    log_dir: ${oc.env:MMF_LOG_DIR,''}
 
     # Directory for saving reports, if not passed a opts based folder will be generated
     # inside save_dir/reports and reports will be saved there
     # Use MMF_REPORT_DIR or env.report_dir to override
-    report_dir: ${oc.env:MMF_REPORT_DIR,}
+    report_dir: ${oc.env:MMF_REPORT_DIR,''}
 
     # Log directory for tensorboard, default points to same as logs
     # Only used when training.tensorboard is enabled.
     # Use MMF_TENSORBOARD_LOGDIR or env.tensorboard_logdir to override
-    tensorboard_logdir: ${oc.env:MMF_TENSORBOARD_LOGDIR,}
+    tensorboard_logdir: ${oc.env:MMF_TENSORBOARD_LOGDIR,''}
 
     # Log directory for Weights and Biases, default points to same as logs
     # Only used when training.wandb is enabled.
     # Use MMF_WANDB_LOGDIR or env.wandb_logdir to override
-    wandb_logdir: ${oc.env:MMF_WANDB_LOGDIR,}
+    wandb_logdir: ${oc.env:MMF_WANDB_LOGDIR,''}
 
     # User directory where user can keep their own models independent of MMF
     # This allows users to create projects which only include MMF as dependency
     # Use MMF_USER_DIR or env.user_dir to specify
-    user_dir: ${oc.env:MMF_USER_DIR,}
+    user_dir: ${oc.env:MMF_USER_DIR,''}
 
 ###
 # Configuration for the distributed setup
@@ -397,6 +397,6 @@ checkpoint:
 
 
 multitasking:
-    enabled: true
+    enabled: false
     type: size_proportional
     params: {}
diff --git a/mmf/configs/models/mmf_transformer/with_audio_video.yaml b/mmf/configs/models/mmf_transformer/with_audio_video.yaml
index 203f90b..483f501 100644
--- a/mmf/configs/models/mmf_transformer/with_audio_video.yaml
+++ b/mmf/configs/models/mmf_transformer/with_audio_video.yaml
@@ -2,31 +2,31 @@ model_config:
   mmf_transformer:
     transformer_base: bert-base-uncased
     modalities:
-      - type: text
-        key: text
-        segment_id: 0
+#      - type: text
+ #       key: text
+  #      segment_id: 0
       - type: video
         key: video
-        embedding_dim: 512
-        segment_id: 1
+        embedding_dim: 768
+        segment_id: 0
         encoder:
-          type: r2plus1d_18
+          type: timesformer #r2plus1d_18
           params:
             num_output_features: 1
             pool_type: avg
             out_dim: 2048
             three_d: true
-      - type: audio
-        key: audio
-        embedding_dim: 512
-        segment_id: 2
-        encoder:
-          type: resnet18_audio
-          params:
-            num_output_features: 1
-            pool_type: avg
-            out_dim: 2048
-            three_d: false
+#      - type: audio
+ #       key: audio
+  #      embedding_dim: 512
+   #     segment_id: 2
+    #    encoder:
+     #     type: resnet18_audio
+      #    params:
+       #     num_output_features: 1
+        #    pool_type: avg
+         #   out_dim: 2048
+          #  three_d: false
     initializer_range: 0.02
     initializer_mean: 0.0
     layer_norm_weight_fill: 1.0
@@ -35,15 +35,8 @@ model_config:
     token_noise_std: 0.01
     token_noise_mean: 0.0
     finetune_lr_multiplier: 1
-    image_encoder:
-      type: resnet152
-      params:
-        pretrained: true
-        pool_type: avg
-        num_output_features: 1
-    freeze_encoder: false
     heads:
       - type: mlp
-        num_labels: ???
+        num_labels: 2
     losses:
     - type: logit_bce
diff --git a/mmf/datasets/processors/bert_processors.py b/mmf/datasets/processors/bert_processors.py
index dd7ffc2..663b06c 100644
--- a/mmf/datasets/processors/bert_processors.py
+++ b/mmf/datasets/processors/bert_processors.py
@@ -205,7 +205,7 @@ class BertTokenizer(MaskedTokenProcessor):
         output = self._convert_to_indices(
             tokens_a, tokens_b, probability=self._probability
         )
-        output["text"] = output["tokens"]
+        output["text"] = output["input_ids"]
         return output
 
 
diff --git a/mmf/datasets/processors/video_processors.py b/mmf/datasets/processors/video_processors.py
index f02b5b2..c920206 100644
--- a/mmf/datasets/processors/video_processors.py
+++ b/mmf/datasets/processors/video_processors.py
@@ -12,6 +12,8 @@ from mmf.datasets.processors import BaseProcessor
 from omegaconf import OmegaConf
 from torchvision import transforms as img_transforms
 
+from PIL import Image
+import numpy as np
 
 logger = logging.getLogger()
 
@@ -184,6 +186,9 @@ class VideoTransforms(BaseProcessor):
 
         self.transform = img_transforms.Compose(transforms_list)
 
+        self.gif_counter = 0
+        self.max_gifs = 5  # Change this value to the desired number of GIFs
+
     def get_transform_object(self, transform_type, transform_params):
         from pytorchvideo import transforms as ptv_transforms
 
@@ -196,8 +201,18 @@ class VideoTransforms(BaseProcessor):
         if transform is not None:
             return self.instantiate_transform(transform, transform_params)
 
+#            return img_transforms.Compose(
+ #           [
+  #              ptv_transforms.Permute((0, 3, 1, 2)),
+   #             self.instantiate_transform(transform, transform_params),
+    #            ptv_transforms.Permute((0, 2, 3, 1))
+     #       ]
+      #  )
+
+
         # 3) torchvision.transforms
         img_transform = getattr(img_transforms, transform_type, None)
+
         assert img_transform is not None, (
             f"transform {transform_type} is not found in pytorchvideo "
             "transforms, processor registry, or torchvision transforms"
@@ -222,9 +237,30 @@ class VideoTransforms(BaseProcessor):
         return transform(*transform_params)
 
     def __call__(self, x):
-        # Support both dict and normal mode
+
+        #print("Shape before transformation:", x.shape)
+        video_frames_np = self.transform(x).cpu().numpy()
+        video_frames_np = video_frames_np.transpose(1, 2, 3, 0)
+        video_frames_np = (video_frames_np * 255).astype(np.uint8)
+        image_list = [Image.fromarray(frame) for frame in video_frames_np]
+
+        # Save the GIF only if the maximum number of GIFs has not been reached
+        if self.gif_counter < self.max_gifs:
+            save_path=f"/home/maria/mmf/save/gifs/save_gif_{self.gif_counter}.gif"
+            image_list[0].save(
+                save_path, 
+                save_all=True, 
+                append_images=image_list[1:], 
+                loop=0,
+                duration=1000
+                )
+            self.gif_counter += 1
+
         if isinstance(x, collections.abc.Mapping):
             x = x["video"]
             return {"video": self.transform(x)}
         else:
-            return self.transform(x)
+            x_transformed=self.transform(x)
+            #For timesformer
+            x_transformed=x_transformed.permute(1, 0, 2, 3) 
+            return x_transformed
diff --git a/mmf/models/mmf_transformer.py b/mmf/models/mmf_transformer.py
index 1d2d1c5..7395793 100644
--- a/mmf/models/mmf_transformer.py
+++ b/mmf/models/mmf_transformer.py
@@ -18,6 +18,8 @@ from mmf.utils.build import build_encoder
 from omegaconf import MISSING, OmegaConf
 from torch import nn, Tensor
 
+from mmf.datasets.builders.viva._utils import img2gif
+
 
 logger = logging.getLogger(__name__)
 
@@ -252,6 +254,10 @@ class MMFTransformer(BaseTransformer):
             # For a feature which is of shape B X D and
             # is not text (which is B X L converted later by embeddings to B X L X D)
             # We convert it to B X 1 X D to signify single position dim.
+            print("self.modality_type[idx]", self.modality_type[idx])
+            print("modality_type keys", self.modality_type.keys())
+            #print(input_ids[modality])
+            print("input_ids[modality].shape",input_ids[modality].shape)
             if self.modality_type[idx] != "text" and input_ids[modality].dim() == 2:
                 input_ids[modality] = input_ids[modality].unsqueeze(1)
 
diff --git a/mmf/models/transformers/backends/huggingface.py b/mmf/models/transformers/backends/huggingface.py
index 130ab54..d0c5263 100644
--- a/mmf/models/transformers/backends/huggingface.py
+++ b/mmf/models/transformers/backends/huggingface.py
@@ -145,6 +145,8 @@ class HuggingfaceEmbeddings(nn.Module):
         ):
             modality_name = self.modality_keys[idx]
             total_embedding = token_emb(tokens_ids[modality_name])
+            print("modality_name", modality_name)
+            print("pos_embedding", pos_emb)
 
             if modality_name in position_ids:
                 total_embedding += pos_emb(position_ids[modality_name])
diff --git a/mmf/modules/encoders.py b/mmf/modules/encoders.py
index 2e6402e..9651283 100644
--- a/mmf/modules/encoders.py
+++ b/mmf/modules/encoders.py
@@ -37,6 +37,7 @@ try:
 except ImportError:
     pass
 
+from .modeling_timesformer import TimesformerModel
 
 logger = logging.getLogger()
 
@@ -800,7 +801,8 @@ class R2Plus1D18VideoEncoder(PooledEncoder):
             pretrained=config.get("pretrained", True)
         )
         modules = list(model.children())[:-2]
-        return nn.Sequential(*modules)
+        output=nn.Sequential(*modules)
+        return output
 
 
 @registry.register_encoder("resnet18_audio")
@@ -851,3 +853,53 @@ class ViTEncoder(Encoder):
             kwargs["output_hidden_states"] = False
         output = self.module(*args, **kwargs)
         return output["last_hidden_state"], output.get("hidden_states", None)
+
+
+#@registry.register_encoder("timesformer")
+#class TimesFormerEncoder(Encoder):
+ #   @dataclass
+  #  class Config(Encoder.Config):
+   #     name: str = "timesformer"
+    #    random_init: bool = False
+     #   gradient_checkpointing: bool = False
+
+#    def __init__(self, config: Config, *args, **kwargs):
+ #       super().__init__()
+  #      self.config = config
+   #     self.module = TimesformerModel.from_pretrained("facebook/timesformer-base-finetuned-k400")
+    #    self.embeddings = self.module.embeddings
+     #   self.out_dim = self.module.config.hidden_size
+
+#    def forward(self, *args, **kwargs):
+ #       if "output_hidden_states" not in kwargs:
+  #          kwargs["output_hidden_states"] = False
+   #     output = self.module(*args, **kwargs)
+    #    return output["last_hidden_state"]
+
+
+@registry.register_encoder("timesformer")
+class TimesFormerEncoder(PooledEncoder):
+    """
+    R2Plus1D based video encoder. Returns back a tensor of dim 2048.
+    By default, pretrained version is used.
+    See https://arxiv.org/abs/1711.11248.
+    """
+
+    @dataclass
+    class Config(PooledEncoder.Config):
+        name: str = "timesformer"
+        out_dim: int = 768  # out dim
+        pretrained: bool = True  # if should use pretrained version or not
+        three_d: bool = True
+
+    def build_encoder(self, config: Config, *args, **kwargs):
+        model = TimesformerModel.from_pretrained("fcakyon/timesformer-base-finetuned-k400")
+        return model
+
+    def forward(self, x: Tensor) -> Tensor:
+        out = self.encoder(x)
+        #out = out["last_hidden_state"]
+        #self.pool = nn.AdaptiveAvgPool1d((1,))
+        #out = out = self.pool(out.transpose(1, 2))
+        #out = out.transpose(1, 2).contiguous()
+        return out
diff --git a/mmf/modules/losses.py b/mmf/modules/losses.py
index da8924a..38a5a25 100644
--- a/mmf/modules/losses.py
+++ b/mmf/modules/losses.py
@@ -245,7 +245,8 @@ class LogitBinaryCrossEntropy(nn.Module):
         """
         scores = model_output["scores"]
         targets = sample_list["targets"]
-        loss = F.binary_cross_entropy_with_logits(scores, targets, reduction="mean")
+        weights = torch.tensor([0.42, 0.58]).cuda()
+        loss = F.binary_cross_entropy_with_logits(scores, targets, reduction="mean", weight=weights)
 
         return loss * targets.size(1)
 
diff --git a/mmf/modules/metrics.py b/mmf/modules/metrics.py
index cc1f754..d8d8387 100644
--- a/mmf/modules/metrics.py
+++ b/mmf/modules/metrics.py
@@ -58,8 +58,13 @@ from sklearn.metrics import (
     precision_recall_curve,
     precision_recall_fscore_support,
     roc_auc_score,
+    confusion_matrix,
 )
+
 from torch import Tensor
+import numpy as np
+import matplotlib.pyplot as plt
+import itertools
 
 
 def _convert_to_one_hot(expected, output):
@@ -240,6 +245,263 @@ class BaseMetric:
         return len(self._dataset_names) == 0 or dataset_name in self._dataset_names
 
 
+@registry.register_metric("confusion_matrix")
+class ConfusionMatrixMetric(BaseMetric):
+    def __init__(self, score_key="scores", target_key="targets", topk=1, num_classes=2, normalize=None):
+        super().__init__("confusion_matrix")
+        self.num_classes = num_classes
+        self.normalize = normalize
+        self.score_key = score_key
+        self.target_key = target_key
+        self.topk = topk
+
+    def calculate(self, sample_list, model_output, *args, **kwargs):
+        """Calculate sensitivity and return it back.
+
+        Args:
+            sample_list (SampleList): SampleList provided by DataLoader for
+                                the current iteration.
+            model_output (Dict): Dict returned by the model.
+
+        Returns:
+            torch.FloatTensor: Sensitivity.
+
+        """
+        output = model_output[self.score_key]
+        batch_size = output.shape[0]
+        expected = sample_list[self.target_key]
+
+        assert (
+            output.dim() <= 2
+        ), "Output from model shouldn't have more than dim 2 for accuracy"
+        assert (
+            expected.dim() <= 2
+        ), "Expected target shouldn't have more than dim 2 for accuracy"
+
+        if output.dim() == 2:
+            output = output.topk(self.topk, 1, True, True)[1].t().squeeze()
+
+        # If more than 1
+        # If last dim is 1, we directly have class indices
+        if expected.dim() == 2 and expected.size(-1) != 1:
+            expected = expected.topk(self.topk, 1, True, True)[1].t().squeeze()
+        
+        # Compute false positives, true positives, false negatives, true negatives
+        FP = ((output == 1) & (expected == 0)).sum().float()
+        FN = ((output == 0) & (expected == 1)).sum().float()
+        TP = ((output == 1) & (expected == 1)).sum().float()
+        TN = ((output == 0) & (expected == 0)).sum().float()
+        predicted_admitted=((output == 1)).sum().float()
+        predicted_discharged=((output == 0)).sum().float()
+        actual_admitted=((expected == 1)).sum().float()
+        actual_discharged=((expected == 0)).sum().float()
+
+        #return {"false_positives": FP, "true_positives": TP, "false_negatives": FN, "true_negatives": TN, 
+         #       "admitted": admitted, "discharged": discharged, "batch_size": batch_size}
+        
+        return {"false_positives": FP, "true_positives": TP, "false_negatives": FN, "true_negatives": TN, 
+                "predicted_admitted": predicted_admitted, "predicted_discharged": predicted_discharged,
+                "actual_admitted": actual_admitted, "actual_discharged": actual_discharged}
+
+@registry.register_metric("specificity")
+class Specificity(BaseMetric):
+    """Metric for calculating specificity (true negative rate).
+
+    **Key:** ``specificity``
+    """
+
+    def __init__(self, score_key="scores", target_key="targets", topk=1):
+        super().__init__("specificity")
+        self.score_key = score_key
+        self.target_key = target_key
+        self.topk = topk
+
+    def calculate(self, sample_list, model_output, *args, **kwargs):
+        """Calculate sensitivity and return it back.
+
+        Args:
+            sample_list (SampleList): SampleList provided by DataLoader for
+                                the current iteration.
+            model_output (Dict): Dict returned by the model.
+
+        Returns:
+            torch.FloatTensor: Sensitivity.
+
+        """
+        output = model_output[self.score_key]
+        expected = sample_list[self.target_key]
+
+        assert (
+            output.dim() <= 2
+        ), "Output from model shouldn't have more than dim 2 for accuracy"
+        assert (
+            expected.dim() <= 2
+        ), "Expected target shouldn't have more than dim 2 for accuracy"
+
+        if output.dim() == 2:
+            output = output.topk(self.topk, 1, True, True)[1].t().squeeze()
+
+        # If more than 1
+        # If last dim is 1, we directly have class indices
+        if expected.dim() == 2 and expected.size(-1) != 1:
+            expected = expected.topk(self.topk, 1, True, True)[1].t().squeeze()
+
+        true_negatives = ((output == 0) & (expected == 0)).sum().float()
+        actual_negatives = (expected == 0).sum().float()
+
+        specificity = true_negatives / (actual_negatives+0.0000000001) 
+        return specificity
+
+
+@registry.register_metric("sensitivity")
+class Sensitivity(BaseMetric):
+    """Metric for calculating sensitivity (true positive rate).
+
+    **Key:** ``sensitivity``
+    """
+
+    def __init__(self, score_key="scores", target_key="targets", topk=1):
+        super().__init__("sensitivity")
+        self.score_key = score_key
+        self.target_key = target_key
+        self.topk = topk
+
+    def calculate(self, sample_list, model_output, *args, **kwargs):
+        """Calculate sensitivity and return it back.
+
+        Args:
+            sample_list (SampleList): SampleList provided by DataLoader for
+                                the current iteration.
+            model_output (Dict): Dict returned by the model.
+
+        Returns:
+            torch.FloatTensor: Sensitivity.
+
+        """
+        output = model_output[self.score_key]
+        expected = sample_list[self.target_key]
+
+        assert (
+            output.dim() <= 2
+        ), "Output from model shouldn't have more than dim 2 for accuracy"
+        assert (
+            expected.dim() <= 2
+        ), "Expected target shouldn't have more than dim 2 for accuracy"
+
+        if output.dim() == 2:
+            output = output.topk(self.topk, 1, True, True)[1].t().squeeze()
+
+        # If more than 1
+        # If last dim is 1, we directly have class indices
+        if expected.dim() == 2 and expected.size(-1) != 1:
+            expected = expected.topk(self.topk, 1, True, True)[1].t().squeeze()
+
+        true_positives = ((output == 1) & (expected == 1)).sum().float()
+        actual_positives = (expected == 1).sum().float()
+
+        sensitivity = true_positives / (actual_positives+0.0000000001) 
+
+        return sensitivity
+
+@registry.register_metric("npv")
+class NPV(BaseMetric):
+    """Metric for calculating NPV (Negative Predictive Value).
+
+    **Key:** ``npv``
+    """
+
+    def __init__(self, score_key="scores", target_key="targets", topk=1):
+        super().__init__("npv")
+        self.score_key = score_key
+        self.target_key = target_key
+        self.topk = topk
+
+    def calculate(self, sample_list, model_output, *args, **kwargs):
+        """Calculate sensitivity and return it back.
+
+        Args:
+            sample_list (SampleList): SampleList provided by DataLoader for
+                                the current iteration.
+            model_output (Dict): Dict returned by the model.
+
+        Returns:
+            torch.FloatTensor: Sensitivity.
+
+        """
+        output = model_output[self.score_key]
+        expected = sample_list[self.target_key]
+
+        assert (
+            output.dim() <= 2
+        ), "Output from model shouldn't have more than dim 2 for accuracy"
+        assert (
+            expected.dim() <= 2
+        ), "Expected target shouldn't have more than dim 2 for accuracy"
+
+        if output.dim() == 2:
+            output = output.topk(self.topk, 1, True, True)[1].t().squeeze()
+
+        # If more than 1
+        # If last dim is 1, we directly have class indices
+        if expected.dim() == 2 and expected.size(-1) != 1:
+            expected = expected.topk(self.topk, 1, True, True)[1].t().squeeze()
+
+        true_negatives = ((output == 0) & (expected == 0)).sum().float()
+        predicted_negatives = (output == 0).sum().float()
+
+        npv = true_negatives / (predicted_negatives+0.0000000001) 
+        return npv
+
+@registry.register_metric("ppv")
+class PPV(BaseMetric):
+    """Metric for calculating PPV (Positive Predictive Value).
+
+    **Key:** ``ppv``
+    """
+
+    def __init__(self, score_key="scores", target_key="targets", topk=1):
+        super().__init__("ppv")
+        self.score_key = score_key
+        self.target_key = target_key
+        self.topk = topk
+
+    def calculate(self, sample_list, model_output, *args, **kwargs):
+        """Calculate sensitivity and return it back.
+
+        Args:
+            sample_list (SampleList): SampleList provided by DataLoader for
+                                the current iteration.
+            model_output (Dict): Dict returned by the model.
+
+        Returns:
+            torch.FloatTensor: Sensitivity.
+
+        """
+        output = model_output[self.score_key]
+        expected = sample_list[self.target_key]
+
+        assert (
+            output.dim() <= 2
+        ), "Output from model shouldn't have more than dim 2 for accuracy"
+        assert (
+            expected.dim() <= 2
+        ), "Expected target shouldn't have more than dim 2 for accuracy"
+
+        if output.dim() == 2:
+            output = output.topk(self.topk, 1, True, True)[1].t().squeeze()
+
+        # If more than 1
+        # If last dim is 1, we directly have class indices
+        if expected.dim() == 2 and expected.size(-1) != 1:
+            expected = expected.topk(self.topk, 1, True, True)[1].t().squeeze()
+
+        true_positives = ((output == 1) & (expected == 1)).sum().float()
+        predicted_positives = (expected == 1).sum().float()
+
+        ppv = true_positives / (predicted_positives+0.0000000001) 
+        return ppv
+
+        
 @registry.register_metric("accuracy")
 class Accuracy(BaseMetric):
     """Metric for calculating accuracy.
@@ -285,6 +547,7 @@ class Accuracy(BaseMetric):
             expected = expected.topk(self.topk, 1, True, True)[1].t().squeeze()
 
         correct = (expected == output.squeeze()).sum().float()
+
         return correct / batch_size
 
 
diff --git a/mmf/trainers/callbacks/logistics.py b/mmf/trainers/callbacks/logistics.py
index 8ec2da7..ace00d7 100644
--- a/mmf/trainers/callbacks/logistics.py
+++ b/mmf/trainers/callbacks/logistics.py
@@ -105,6 +105,7 @@ class LogisticsCallback(Callback):
                 ),
             }
         )
+        
         self.train_timer.reset()
         summarize_report(
             current_iteration=self.trainer.current_iteration,
@@ -113,13 +114,15 @@ class LogisticsCallback(Callback):
             meter=kwargs["meter"],
             extra=extra,
             tb_writer=self.tb_writer,
-            wandb_logger=self.wandb_logger,
+            wandb_logger=self.wandb_logger
         )
 
+
     def on_validation_start(self, **kwargs):
         self.snapshot_timer.reset()
 
     def on_validation_end(self, **kwargs):
+
         max_updates = getattr(self.trainer, "max_updates", None)
         num_updates = getattr(self.trainer, "num_updates", None)
         extra = {
@@ -131,6 +134,7 @@ class LogisticsCallback(Callback):
         }
         extra.update(self.trainer.early_stop_callback.early_stopping.get_info())
         self.train_timer.reset()
+
         summarize_report(
             current_iteration=self.trainer.current_iteration,
             num_updates=num_updates,
@@ -138,7 +142,7 @@ class LogisticsCallback(Callback):
             meter=kwargs["meter"],
             extra=extra,
             tb_writer=self.tb_writer,
-            wandb_logger=self.wandb_logger,
+            wandb_logger=self.wandb_logger
         )
 
     def on_test_end(self, **kwargs):
diff --git a/mmf/utils/logger.py b/mmf/utils/logger.py
index 629c443..8d30fdf 100644
--- a/mmf/utils/logger.py
+++ b/mmf/utils/logger.py
@@ -219,7 +219,7 @@ def summarize_report(
     should_print=True,
     extra=None,
     tb_writer=None,
-    wandb_logger=None,
+    wandb_logger=None
 ):
     if extra is None:
         extra = {}
@@ -238,7 +238,9 @@ def summarize_report(
 
     if wandb_logger:
         metrics = meter.get_scalar_dict()
-        wandb_logger.log_metrics({**metrics, "trainer/global_step": current_iteration})
+        wandb_logger.log_metrics({**metrics, "trainer/global_step": current_iteration})#, "epoch": extra["epoch"]})
+        #wandb_logger.log_metrics({"epoch": extra["epoch"], "trainer/global_step": current_iteration})
+        #wandb_logger.log_metrics({**metrics, "trainer/epoch": extra["epoch"]})
 
     if not should_print:
         return
@@ -475,6 +477,13 @@ class WandbLogger:
 
         self._wandb.log(metrics, commit=commit)
 
+    def confusion_matrix(self, metrics: Dict[str, float], commit=True):
+
+        if not self._should_log_wandb():
+            return
+
+        self._wandb.sklearn.plot_confusion_matrix(y_test, y_pred, nb.classes_) 
+
     def log_model_checkpoint(self, model_path):
         """
         Log the model checkpoint to the wandb dashboard.
