diff --git a/mmf/configs/defaults.yaml b/mmf/configs/defaults.yaml
index 256bd16..3404f79 100644
--- a/mmf/configs/defaults.yaml
+++ b/mmf/configs/defaults.yaml
@@ -16,7 +16,7 @@ training:
     # and generating reports
     experiment_name: mmf_video_audio
     # Maximum number of iterations the training will run
-    max_updates: 22000
+    max_updates: 50
     # Maximum epochs in case you don't want to use max_updates
     # Can be mixed with max iterations, so it will stop whichever is
     # completed first. Default: null means epochs won't be used
@@ -25,7 +25,7 @@ training:
     # After `log_interval` updates, current update's training loss will be
     # reported. This will also report validation on a single batch from validation set
     # to provide an estimate on validation side
-    log_interval: 1
+    log_interval: 100
     # Level of logging, only logs which are >= to current level will be logged
     logger_level: info
     # Log format: json, simple
@@ -104,7 +104,7 @@ training:
 
     early_stop:
         # Whether to use early stopping, (Default: false)
-        enabled: false
+        enabled: true
         # Patience for early stoppings
         patience: 4000
         # Criteria to be monitored for early stopping
diff --git a/mmf/configs/models/viva/defaults.yaml b/mmf/configs/models/viva/defaults.yaml
index 8f90806..315754f 100644
--- a/mmf/configs/models/viva/defaults.yaml
+++ b/mmf/configs/models/viva/defaults.yaml
@@ -19,7 +19,7 @@ model_config:
         three_d: false
     tabular_transforms: 
       in_dim: 2
-      out_dim: 512
+      out_dim: 12 #512
     text_encoder: 
       type: transformer
     interaction_layer: 
@@ -30,7 +30,7 @@ model_config:
       type: mlp
       num_labels: 2
       params:
-        in_dim: 1024
+        in_dim: 12 #512 #1024
         out_dim: 2
     dropout: 0.15
 
diff --git a/mmf/models/viva.py b/mmf/models/viva.py
index ddb57c1..392cd5a 100644
--- a/mmf/models/viva.py
+++ b/mmf/models/viva.py
@@ -53,9 +53,9 @@ class Viva_model(BaseModel):
         tabular_features = self.tabular_transform(tabular)
         tabular_features = self.tabular_batchnorm(tabular_features)
 
-        combined = torch.cat([audio_features, video_features, text_features, tabular_features], dim=1)
-        interacted = self.interaction_layer(combined)
-        interacted = self.dropout(interacted)
+        #combined = torch.cat([audio_features, video_features, text_features, tabular_features], dim=1)
+        #interacted = self.interaction_layer(combined)
+        interacted = self.dropout(tabular_features)
         scores = self.classifier(interacted)
 
         return {"scores": scores}
\ No newline at end of file
diff --git a/mmf/utils/logger.py b/mmf/utils/logger.py
index 8d30fdf..6fa3094 100644
--- a/mmf/utils/logger.py
+++ b/mmf/utils/logger.py
@@ -477,13 +477,6 @@ class WandbLogger:
 
         self._wandb.log(metrics, commit=commit)
 
-    def confusion_matrix(self, metrics: Dict[str, float], commit=True):
-
-        if not self._should_log_wandb():
-            return
-
-        self._wandb.sklearn.plot_confusion_matrix(y_test, y_pred, nb.classes_) 
-
     def log_model_checkpoint(self, model_path):
         """
         Log the model checkpoint to the wandb dashboard.
diff --git a/projects/viva/direct.yaml b/projects/viva/direct.yaml
index ee60a09..81c5886 100644
--- a/projects/viva/direct.yaml
+++ b/projects/viva/direct.yaml
@@ -4,21 +4,24 @@ includes:
 optimizer:
   type: adam_w
   params:
-    lr: 5e-7 #5e-5
-    eps: 1e-10 #1e-8
+    lr: 5e-5 #5e-5
+    eps: 1e-8 #1e-8
 
 scheduler:
   type: warmup_cosine
   params:
-    num_warmup_steps: 15
-    num_training_steps: 150
+    num_warmup_steps: 5
+    num_training_steps: 50
 
 training:
   batch_size: 64
-  lr_scheduler: true
+  lr_scheduler: false
   # Don't forget to update schedule_attributes if you update this
   max_updates: 50
   find_unused_parameters: true
   early_stop:
+    enabled: true
+    patience: 2
     criteria: Viva/logit_bce
     minimize: true
+
